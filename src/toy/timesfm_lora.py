###########################################################################################################
# import libraries
###########################################################################################################

import time
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from peft import LoraConfig, get_peft_model
from timesfm import TimesFM_2p5_200M_torch, ForecastConfig
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from config import *
from util import *

###########################################################################################################
# set configurations
###########################################################################################################

device      = "cuda" if torch.cuda.is_available() else "cpu"
path        = DATA_PATH[DATA]
max_context = DATASET[DATA]['context']
max_horizon = DATASET[DATA]['horizon']
target_col  = DATASET[DATA]['target_col']
ft_len      = int(17420 * 0.7)

raw_df = pd.read_csv(path)
target = raw_df[target_col].values.astype(np.float32)

tr_data = target[:ft_len] 
te_data = target[ft_len:] 

###########################################################################################################
# run TimesFM (Base Model)
###########################################################################################################

print(f"Loading Base TimesFM 2.5 on {device}...")
model = TimesFM_2p5_200M_torch.from_pretrained(MODEL_VER)
model.compile(ForecastConfig(max_context=max_context, 
                             max_horizon=max_horizon, 
                             use_continuous_quantile_head=True, 
                             normalize_inputs=True))

print("ğŸš€ Predicting with TimesFM (Base)...")
start_inf_base = time.time()
base_preds, base_actuals = sliding_window_forecast(model, te_data, max_context, max_horizon)
end_inf_base = time.time() - start_inf_base
print(f"Base Model Inference Time: {end_inf_base:.2f}s")

###########################################################################################################
# run TimesFM + LoRA
###########################################################################################################

PATCH_SIZE = 64 

lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    target_modules=["qkv_proj", "out", "ff0", "ff1"],
    lora_dropout=0.1,
    bias="none"
)

print("\nğŸ› ï¸ Applying LoRA to the model...")
model.model = get_peft_model(model.model, lora_config)
model.model.to(device)
model.model.print_trainable_parameters()

# [â­ ì—ëŸ¬ í•´ê²°ì˜ í•µì‹¬] í† í¬ë‚˜ì´ì € ì…ë ¥ ê¸°ëŒ€ì¹˜ ê°•ì œ ì¡°ì •
base_model = model.model.get_base_model()
if hasattr(base_model, 'tokenizer'):
    # 128 ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì…ë ¥ í”¼ì²˜ë¥¼ 64ë¡œ ê°•ì œ ê³ ì •
    base_model.tokenizer.hidden_layer.in_features = PATCH_SIZE
    # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ì¶° ë‚´ë¶€ ì„¤ì • ì—…ë°ì´íŠ¸
    tgt_context = ((max_context + PATCH_SIZE - 1) // PATCH_SIZE) * PATCH_SIZE
    base_model.tokenizer.context_len = tgt_context

class TimeSeriesDataset(Dataset):
    def __init__(self, data, cl, hl):
        self.data, self.cl, self.hl = data, cl, hl
    def __len__(self):
        return len(self.data) - self.cl - self.hl
    def __getitem__(self, idx):
        return torch.tensor(self.data[idx : idx + self.cl]), torch.tensor(self.data[idx + self.cl : idx + self.cl + self.hl])

train_loader = DataLoader(TimeSeriesDataset(tr_data, max_context, max_horizon), batch_size=32, shuffle=True)
optimizer = optim.AdamW(model.model.parameters(), lr=1e-4)
criterion = nn.MSELoss()

print(f"ğŸ‹ï¸ Training LoRA with tr_data (Context: {max_context})...")
model.model.train()
start_train_lora = time.time()

for epoch in range(5): 
    total_loss = 0
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # [ë‹¨ê³„ 1] 64 ë°°ìˆ˜ íŒ¨ë”©
        curr_len = batch_x.shape[1]
        tgt_len = ((curr_len + PATCH_SIZE - 1) // PATCH_SIZE) * PATCH_SIZE
        
        if curr_len != tgt_len:
            pad_len = tgt_len - curr_len
            padding = torch.zeros((batch_x.shape[0], pad_len), device=device)
            batch_x_padded = torch.cat([padding, batch_x], dim=1)
            masks = torch.ones_like(batch_x_padded).to(device)
            masks[:, :pad_len] = 0
        else:
            batch_x_padded = batch_x
            masks = torch.ones_like(batch_x_padded).to(device)

        # [ë‹¨ê³„ 2] ë°ì´í„°ë¥¼ [Batch, Num_Patches, 64] êµ¬ì¡°ë¡œ Reshape
        num_patches = tgt_len // PATCH_SIZE
        batch_x_input = batch_x_padded.view(batch_x.shape[0], num_patches, PATCH_SIZE)
        masks_input = masks.view(batch_x.shape[0], num_patches, PATCH_SIZE)

        optimizer.zero_grad()
        
        # [ë‹¨ê³„ 3] ëª¨ë¸ í˜¸ì¶œ
        outputs = model.model(batch_x_input, masks_input)
        
        # [ë‹¨ê³„ 4] ì¶œë ¥ ì²˜ë¦¬
        if isinstance(outputs, tuple): outputs = outputs[0]
        if outputs.ndim == 4: # ë¶„ìœ„ìˆ˜ ì°¨ì› í‰ê· 
            outputs = outputs.mean(dim=-1) 
        
        # ì „ì²´ ì‹œí€€ìŠ¤ í¼ì¹˜ê¸° ë° ë§ˆì§€ë§‰ êµ¬ê°„ ìŠ¬ë¼ì´ì‹±
        outputs = outputs.reshape(batch_x.shape[0], -1)
        outputs = outputs[:, -max_horizon:]
        
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    print(f"Epoch {epoch+1}/5 | Loss: {total_loss/len(train_loader):.6f}")

train_time_lora = time.time() - start_train_lora
print(f"âœ… LoRA Training Complete: {train_time_lora:.2f}s")

###########################################################################################################
# Prediction & Evaluation
###########################################################################################################

print("ğŸš€ Predicting with LoRA Enhanced Model...")
model.model.eval()
start_inf_lora = time.time()
lora_preds, _ = sliding_window_forecast(model, te_data, max_context, max_horizon)
end_inf_lora = time.time() - start_inf_lora

base_metrics = calculate_metrics(base_actuals, base_preds)
lora_metrics = calculate_metrics(base_actuals, lora_preds)

print("\n" + "="*60)
print(f"{'Metric':<15} | {'Base Model':<15} | {'LoRA Model':<15}")
print("-" * 60)
m_names = ["MAE", "MSE", "RMSE", "MAPE(%)"]
for i in range(4):
    print(f"{m_names[i]:<15} | {base_metrics[i]:<15.4f} | {lora_metrics[i]:<15.4f}")
print("="*60)

###########################################################################################################
# Visualization
###########################################################################################################

plt.figure(figsize=(15, 7))
plt.plot(base_actuals[:500], label="Actual", color='black', alpha=0.4)
plt.plot(base_preds[:500], label="Base TimesFM", color='blue', linestyle='--')
plt.plot(lora_preds[:500], label="LoRA Enhanced", color='red', alpha=0.7)
plt.title(f"TimesFM 2.5 vs LoRA Enhanced Comparison")
plt.xlabel("Time Step")
plt.ylabel(target_col)
plt.legend()
plt.grid(True, alpha=0.2)
plt.show()