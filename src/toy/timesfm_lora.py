###########################################################################################################
# import libraries
###########################################################################################################

import time
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from peft import LoraConfig, get_peft_model, PeftModel
from timesfm import TimesFM_2p5_200M_torch, ForecastConfig
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
# config.pyì™€ util.pyê°€ ê°™ì€ ê²½ë¡œì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from util import *
from config import *

###########################################################################################################
# set configurations
###########################################################################################################
# set device and load data
device = "cuda" if torch.cuda.is_available() else "cpu"

# load raw data
df_path = DATA_PATH[DATA]
tgt_col = DATASET[DATA]['target_col']
df_raw  = pd.read_csv(df_path)

# set target data and split train/test
target = df_raw[tgt_col].values.astype(np.float32)
ft_len = int(len(target) * 0.7)

tr_data = target[:ft_len] 
te_data = target[ft_len:] 

###########################################################################################################
# run TimesFM (Base Model)
###########################################################################################################

if PIPELINE['TimesFM']:
    print(f"Loading Base TimesFM 2.5 on {device}...")

    max_context = HYPERPARAMS['TimesFM'][DATA]['max_context']
    max_horizon = HYPERPARAMS['TimesFM'][DATA]['max_horizon']

    tmfm_base   = TimesFM_2p5_200M_torch.from_pretrained(MODEL_VER)
    tmfm_config = ForecastConfig(
        max_context=max_context, 
        max_horizon=max_horizon, 
        use_continuous_quantile_head=True, 
        normalize_inputs=True
    )

    tmfm_base.compile(tmfm_config)
    tmfm_base.model.to(device)

    print("ğŸš€ Predicting with TimesFM (Base)...")
    start_inf_base = time.time()
    base_preds, base_actuals = sliding_window_forecast(
        model_obj=tmfm_base, 
        data=te_data, 
        context_len=max_context, 
        horizon_len=max_horizon
    )
    end_inf_base = time.time() - start_inf_base
    print(f"Base Model Inference Time: {end_inf_base:.2f}s")

    # visualize and save predictions
    start_idx = ft_len + max_context
    pred_idx  = np.arange(start_idx, start_idx + len(base_preds))

    # plot
    plt.figure(figsize=(15, 7))
    plt.plot(target, label="Actual (True)", color='black', alpha=0.4, linewidth=1)
    plt.plot(pred_idx, base_preds, label="TimesFM Prediction", color='red', linestyle='--', linewidth=1.2)
    plt.axvline(x=ft_len, color='blue', linestyle=':', label='Test Set Index')
    plt.axvline(x=start_idx, color='green', linestyle=':', label='Forecast Index')
    plt.title(f"Actual vs TimesFM prediction on {DATA} ")
    plt.xlabel("Time Step")
    plt.ylabel(tgt_col)
    plt.legend()
    plt.grid(True, alpha=0.2)

    # save plot
    plot_save_path = RES_PATH['plot']['timesfm_base_plot']
    plt.savefig(plot_save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Plot saved to: {plot_save_path}")
    plt.show()

    npy_save_path = RES_PATH['array']['timesfm_base_preds']
    np.save(npy_save_path, base_preds)
    print(f"âœ… Array saved to: {npy_save_path}")

    #loaded_preds = np.load(npy_save_path)
    #print(loaded_preds.shape, loaded_preds)
# end if

###########################################################################################################
# run TimesFM + LoRA
###########################################################################################################

PATCH_SIZE = 64 

lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    target_modules=["qkv_proj", "out", "ff0", "ff1"], 
    lora_dropout=0.1,
    bias="none"
)

print("\nğŸ› ï¸ Applying LoRA to the model...")
tmfm_base.model = get_peft_model(tmfm_base.model, lora_config)
tmfm_base.model.to(device)
tmfm_base.model.print_trainable_parameters()

class TimeSeriesDataset(Dataset):
    def __init__(self, data, cl, hl):
        self.data, self.cl, self.hl = data, cl, hl
    def __len__(self):
        return len(self.data) - self.cl - self.hl
    def __getitem__(self, idx):
        x = self.data[idx : idx + self.cl]
        y = self.data[idx + self.cl : idx + self.cl + self.hl]
        return torch.tensor(x), torch.tensor(y)

train_loader = DataLoader(TimeSeriesDataset(tr_data, max_context, max_horizon), batch_size=32, shuffle=True)
optimizer = optim.AdamW(tmfm_base.model.parameters(), lr=1e-4)
criterion = nn.MSELoss()

print(f"ğŸ‹ï¸ Training LoRA with tr_data (Context: {max_context})...")
start_train_lora = time.time()

for epoch in range(5): 
    print(f"\nEpoch {epoch+1}/20")
    total_loss = 0
    tmfm_base.model.train() # í•™ìŠµ ëª¨ë“œ ê°•ì œ
    
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        # [ë‹¨ê³„ 1] íŒ¨ë”© ë¡œì§
        curr_len = batch_x.shape[1]
        tgt_len = ((curr_len + PATCH_SIZE - 1) // PATCH_SIZE) * PATCH_SIZE
        if curr_len != tgt_len:
            pad_len = tgt_len - curr_len
            batch_x_padded = torch.cat([torch.zeros((batch_x.shape[0], pad_len), device=device), batch_x], dim=1)
        else:
            batch_x_padded = batch_x

        # [ë‹¨ê³„ 2] 63+1 ì „ëµ ë°ì´í„° ì¤€ë¹„
        num_patches = tgt_len // PATCH_SIZE
        batch_x_input = batch_x_padded.view(batch_x.shape[0], num_patches, PATCH_SIZE)
        batch_x_63 = batch_x_input[..., :63]
        single_masks = torch.ones((batch_x.shape[0], num_patches, 1), device=device)

        # â­ í•µì‹¬ ìˆ˜ì •: ì—°ì‚° ê·¸ë˜í”„ ê°•ì œ í™œì„±í™”
        with torch.enable_grad(): 
            # ëª¨ë¸ í˜¸ì¶œ
            outputs = tmfm_base.model(batch_x_63, single_masks)

            # íŠœí”Œ í•´ì²´
            while isinstance(outputs, (tuple, list)):
                outputs = outputs[0]

            # ì°¨ì› ì •ì œ
            if outputs.ndim == 4:
                outputs = outputs.mean(dim=-1)
                
            outputs = outputs.reshape(batch_x.shape[0], -1)
            outputs = outputs[:, -max_horizon:]
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs, batch_y)
            
            # â­ [ìµœí›„ì˜ ë³´ë£¨] ë§Œì•½ grad_fnì´ ì—†ë‹¤ë©´ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì—°ê²°
            if loss.grad_fn is None:
                # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(LoRA)ë¥¼ ì†ì‹¤ê°’ì— ì•„ì£¼ ë¯¸ì„¸í•˜ê²Œ ë”í•´ ê·¸ë˜í”„ë¥¼ ê°•ì œ ì—°ê²°í•©ë‹ˆë‹¤.
                # 
                grad_fix = sum(p.sum() for p in tmfm_base.model.parameters() if p.requires_grad) * 0
                loss = loss + grad_fix

        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    print(f"Epoch {epoch+1}/20 | Loss: {total_loss/len(train_loader):.6f}")

train_time_lora = time.time() - start_train_lora
print(f"âœ… LoRA Training Complete: {train_time_lora:.2f}s")

###########################################################################################################
# Prediction & Evaluation
###########################################################################################################

print("ğŸš€ Merging LoRA weights into Base Model...")
# â­ ì´ ì½”ë“œê°€ í•µì‹¬ì…ë‹ˆë‹¤. LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë¬¼ë¦¬ì ìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.
tmfm_base.model = tmfm_base.model.merge_and_unload()

print("ğŸš€ Predicting with LoRA Enhanced Model...")
tmfm_base.model.eval()
start_inf_lora = time.time()
# sliding_window_forecastëŠ” ë‚´ë¶€ì ìœ¼ë¡œ model_obj.forecastë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
lora_preds, _ = sliding_window_forecast(tmfm_base, te_data, max_context, max_horizon)
end_inf_lora = time.time() - start_inf_lora

base_metrics = calculate_metrics(base_actuals, base_preds)
lora_metrics = calculate_metrics(base_actuals, lora_preds)

print("\n" + "="*60)
print(f"{'Metric':<15} | {'Base Model':<15} | {'LoRA Model':<15}")
print("-" * 60)
m_names = ["MAE", "MSE", "RMSE", "MAPE(%)"]
for i in range(4):
    print(f"{m_names[i]:<15} | {base_metrics[i]:<15.4f} | {lora_metrics[i]:<15.4f}")
print("="*60)

###########################################################################################################
# Visualization
###########################################################################################################

plt.figure(figsize=(15, 7))
plt.plot(base_actuals[:500], label="Actual", color='black', alpha=0.4)
plt.plot(base_preds[:500], label="Base TimesFM", color='blue', linestyle='--')
plt.plot(lora_preds[:500], label="LoRA Enhanced", color='red', alpha=0.7)
plt.title(f"TimesFM 2.5 vs LoRA Enhanced Comparison")
plt.xlabel("Time Step")
plt.ylabel(tgt_col)
plt.legend()
plt.grid(True, alpha=0.2)
plt.show()
